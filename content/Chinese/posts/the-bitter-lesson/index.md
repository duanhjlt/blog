---
keywords: ['blog']
title: '苦涩的教训'
date: 2025-06-30T16:06:44+08:00
lastmod: 2025-06-30T16:06:44+08:00
draft: false
description: 
author: 'duanhongjin'
tags: ['翻译']
categories: []
tocLevels: ["h2", "h3", "h4"]
comments: true # 本页面是否显示评论
reward: true # 打赏
showToc: true # 显示目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示路径
---

> 原文：[The Bitter Lesson](https://www.artfintel.com/p/the-bitter-lesson)

《苦涩的教训》是一篇非常出色但被广泛误解的文章。这篇文章的重点是，随着时间的推移，那些能够受益于计算能力扩展的方法，将最终胜过那些不能的方法。

这篇文章的观点不是：

- 我们永远不应该融合人类知识。
- 我们所需要的仅仅是深度学习和规模化（实际上，作者 Rich Sutton 对深度学习持相对怀疑的态度）。
  
整篇文章的核心在于，在过去的五十年里，我们整个行业所能获得的计算量（算力）已经有了巨大的增长，并且我们预计AI研究可用的算力还将继续大规模增加。那些懂得利用算力的方法将从中受益，而那些不懂得利用的则将受其所累。

这个教训之所以“苦涩”，是因为通过融合人类知识来获得结果，通常要容易和快捷得多。

如果你在1995年训练一个自动补全系统，使用“下一个词元预测”（next token prediction）可能不会有太大进展，相反，手写的或者基于统计生成的规则会表现得更好。到了2005年，N-gram模型是最佳选择。直到2010年代中期，我们才开始看到深度学习在自然语言处理（NLP）领域占据主导地位，而直到2010年代末，自监督学习才成为主流。在这条路上的每一步，融合人类知识都曾是有利的，并且是你超越竞争对手的一种方式。但从长远来看，这是一条死胡同。在足够长的时间范围内，那些利用更多算力的方法会表现得更出色。算力是我们唯一可以预期会增长数个数量级的参数。尽管我多么希望情况不是这样，但我们拥有的token数量在未来增长1000倍是不太可能的，而对于算力来说，这几乎是必然的。

一个典型的例子是计算机象棋。在“深蓝”（Deep Blue）出现之前，专家系统被广泛使用。“深蓝”证明了，利用算力针对一个手写的价值函数¹进行大规模搜索，可以表现得极其出色。“深蓝”是“规模化算力”/计算机搜索阵营的一次巨大胜利，因为它更多地基于规模而非人类的启发式规则。但它仍然需要一个由人类专家创建的、包含8000个自定义象棋特征的评估函数，并且该评估函数使用手动选择的权重来对这些特征进行加权。衡量一个系统通用性的一个标准是，将其扩展到不同场景的难易程度。将“深蓝”扩展到围棋上会极具挑战性，因为人们需要通过创建另外8000个自定义的围棋特征，才能得出一个合适的评估函数。

计算机围棋是人类知识不足的另一个例子。AlphaGo Zero与当时最先进的围棋机器人进行了对战，包括Pachi、GnuGo和CrazyStone。Pachi和CrazyStone使用的是带有启发式价值函数的蒙特卡洛树搜索（MCTS），而GnuGo则是一个专家系统，用一个手工创建的决策树来选择棋步。它们在当时很出色！但它们最终都成了死胡同。正如Rich在文章中所述：

> 苦涩的教训基于以下历史观察：
> 
> 1）AI研究人员总是试图将知识构建到他们的智能体中。
>
> 2）这在短期内总是有帮助的，并且能给研究人员带来个人满足感。
>
> 3）但从长远来看，它会达到一个平台期，甚至会抑制未来的进步。
>
> 4）最终的突破性进展来自于一种相反的方法，该方法基于通过搜索和学习来扩展计算。
>
> 最终的成功带有一丝苦涩，并且常常未被完全消化，因为这是对一种受人偏爱的、以人类为中心的方法的胜利。

如果你去看GnuGo的代码，你会发现它凝聚了很多人的辛勤工作，但其效果却远比可能达到的水平要差得多。令人惊讶的是，尽管GnuGo始于1989年，但其版本更新一直持续到2009年。所以，其作者们无疑知道“深蓝”以及规模化搜索取得的惊人胜利，但他们仍然继续推进他们的专家系统。曾在谷歌大脑（Google Brain）复现了AlphaGo的前研究员Brian Lee，对此提供了一个令人信服的解释：

> 我想提出另一点：[苦涩教训的]这些阶段是以十年左右的时间跨度发生的。在这十年间，博士学位被授予，职业身份被建立，晋升标准被设定，文化被定义，组织结构被固化。就像科学的进步伴随着一场又一场的葬礼一样，难题的进展也伴随着一个又一个组织的关闭。

再想一个场景。你在一个大语言模型（LLM）实验室工作，你必须让你的基准测试分数超过竞争对手，否则你就会被解雇。你面临着一个直接的诱惑，那就是引入人类知识，在这种情况下，可能就是为特定基准准备的专门数据集。

一个更好的方法是让模型在通用性上变得更强。将“专注于那些能随算力扩展的方法”作为一个筛选标准，是一个强有力的赌注，因为黄仁勋（Jensen Huang）正在尽其所能为你提供多个数量级的更多算力（FLOPS）。像测试时计算（test time compute）、合成数据（synthetic data）或混合专家模型（MoE models）都是很好的例子。但是这种方法的问题在于（当我写下来时感觉很明显），在当下，它让人觉得是一种奢侈。我们没有时间去做严谨的科学研究，我们必须在LiveCodeBench上击败其他实验室。这就是那苦涩的教训：DeepSeek专注于通用能力的提升，让这些方法生效，将它们扩展到3.8e25 FLOPS的算力，并达到了最先进水平（SOTA）。

我最近在读的文章：

- 《接下来是什么》（What comes next），作者Nathan Lambert (Interconnects)，其中讨论了O3模型的卓越之处等方面。
- 《R1中的欠训练词元》（Undertrained tokens in R1），作者Sander Land。
- 《深蓝》的论文，值得一读。